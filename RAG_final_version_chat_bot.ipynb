{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75a75f42-4416-4666-9261-86a6d4f05a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: D:\\projects\\llm_engineering\\week5\\Project_hochschule_kaiserslautern\n",
      "\n",
      "==================================================\n",
      "Processing folder: Data/contacts (type: contacts)\n",
      "==================================================\n",
      "  ✓ Loaded 1 units from pruefungsamt_rag.json\n",
      "\n",
      "==================================================\n",
      "Processing folder: Data/extra (type: extra)\n",
      "==================================================\n",
      "  ✓ Loaded 1 units from Deutschland-Semesterticket.md\n",
      "\n",
      "==================================================\n",
      "Processing folder: Data/klausur (type: klausur)\n",
      "==================================================\n",
      "  ✓ Loaded 3 units from Pruefungsplan_PO19.pdf\n",
      "\n",
      "==================================================\n",
      "Processing folder: Data/professoren (type: professoren)\n",
      "==================================================\n",
      "  ✓ Loaded 1 units from Albert Meij.md\n",
      "  ✓ Loaded 1 units from Andreas Steil.md\n",
      "  ✓ Loaded 1 units from arl-Heinz Helmstädter.md\n",
      "  ✓ Loaded 1 units from Christian M. Thurnes.md\n",
      "  ✓ Loaded 1 units from Christian Schumann.md\n",
      "  ✓ Loaded 1 units from Constantin Bauer.md\n",
      "  ✓ Loaded 1 units from Dirk Enk.md\n",
      "  ✓ Loaded 1 units from Eva Maria Kiss.md\n",
      "  ✓ Loaded 1 units from Felix Möhrle.md\n",
      "  ✓ Loaded 1 units from Felix Wick.md\n",
      "  ✓ Loaded 1 units from Frank Bomarius.md\n",
      "  ✓ Loaded 1 units from Gerd Bitsch.md\n",
      "  ✓ Loaded 1 units from habil. Peter Starke.md\n",
      "  ✓ Loaded 1 units from Hans-Peter Geromiller.md\n",
      "  ✓ Loaded 1 units from Hartmut Opperskalski.md\n",
      "  ✓ Loaded 1 units from Heiko Heß.md\n",
      "  ✓ Loaded 1 units from Hubert Klein.md\n",
      "  ✓ Loaded 1 units from Karsten Glöser.md\n",
      "  ✓ Loaded 1 units from Klaus Fischer.md\n",
      "  ✓ Loaded 1 units from Martin Böhm.md\n",
      "  ✓ Loaded 1 units from Martin Hoof.md\n",
      "  ✓ Loaded 1 units from Matthias Hampel.md\n",
      "  ✓ Loaded 1 units from Matthias R. Leiner.md\n",
      "  ✓ Loaded 1 units from Michael Herchenhan.md\n",
      "  ✓ Loaded 1 units from Michael Magin.md\n",
      "  ✓ Loaded 1 units from Norbert Gilbert.md\n",
      "  ✓ Loaded 1 units from Oliver Maier.md\n",
      "  ✓ Loaded 1 units from Peter Heidrich.md\n",
      "  ✓ Loaded 1 units from Stefan Steidel.md\n",
      "  ✓ Loaded 1 units from Steffen Schütz.md\n",
      "  ✓ Loaded 1 units from Stephan Werth.md\n",
      "  ✓ Loaded 1 units from Sven Urschel.md\n",
      "  ✓ Loaded 1 units from Thomas Kilb.md\n",
      "  ✓ Loaded 1 units from Thomas Reiner.md\n",
      "  ✓ Loaded 1 units from Torsten Hielscher.md\n",
      "  ✓ Loaded 1 units from Victor López López.md\n",
      "  ✓ Loaded 1 units from Wulf Kaiser.md\n",
      "\n",
      "==================================================\n",
      "Processing folder: Data/Termin (type: Termin)\n",
      "==================================================\n",
      "  ✓ Loaded 1 units from termin.md\n",
      "\n",
      "==================================================\n",
      "Processing folder: Data/Vorpraktikum (type: Vorpraktikum)\n",
      "==================================================\n",
      "  ✓ Loaded 1 units from Grundpraktikum.md\n",
      "  ✓ Loaded 1 units from Nachweis_Vorpraktikum_D_Hinweise.pdf\n",
      "  ✓ Loaded 7 units from Vorpraktikumsordnung_aing_2019.pdf\n",
      "\n",
      "==================================================\n",
      "Total documents loaded: 52\n",
      "==================================================\n",
      "Total number of chunks: 173\n",
      "\n",
      "Documents per type:\n",
      "  - contacts: 1 docs → 21 chunks\n",
      "  - extra: 1 docs → 4 chunks\n",
      "  - klausur: 3 docs → 28 chunks\n",
      "  - professoren: 37 docs → 90 chunks\n",
      "  - Termin: 1 docs → 2 chunks\n",
      "  - Vorpraktikum: 9 docs → 28 chunks\n",
      "Deleted: vector_Kaiserslautern_db\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from collections import Counter\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.manifold import TSNE\n",
    "import gradio as gr\n",
    "import tqdm as notebook_tqdm\n",
    "# Modern LangChain 0.3 Imports\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader, CSVLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_core.callbacks import StdOutCallbackHandler\n",
    "\n",
    "# price is a factor for our company, so we're going to use a low cost model\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "db_name = \"vector_Kaiserslautern_db\"\n",
    "\n",
    "# Load environment variables in a file called .env\n",
    "load_dotenv(override=True)\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.7, model_name=MODEL)\n",
    "\n",
    "\n",
    "def add_metadata(doc, doc_type):\n",
    "    doc.metadata[\"doc_type\"] = doc_type\n",
    "    return doc\n",
    "\n",
    "documents = []\n",
    "\n",
    "# Folders to process\n",
    "folders_to_process = [\n",
    "    \"Data/contacts\", \n",
    "    \"Data/extra\", \n",
    "    \"Data/klausur\", \n",
    "    \"Data/professoren\", \n",
    "    \"Data/Termin\", \n",
    "    \"Data/Vorpraktikum\"\n",
    "]\n",
    "\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "for folder in folders_to_process:\n",
    "    if not os.path.exists(folder):\n",
    "        print(f\"Skipping (doesn't exist): {folder}\")\n",
    "        continue\n",
    "    \n",
    "    doc_type = os.path.basename(folder)\n",
    "    print(f\"\\n{'='*50}\\nProcessing folder: {folder} (type: {doc_type})\\n{'='*50}\")\n",
    "    \n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        if '.ipynb_checkpoints' in root: continue\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                if file.endswith('.pdf'):\n",
    "                    loader = PyPDFLoader(file_path)\n",
    "                elif file.endswith(('.md', '.txt', '.json')):\n",
    "                    loader = TextLoader(file_path, encoding='utf-8')\n",
    "                elif file.endswith('.csv'):\n",
    "                    loader = CSVLoader(file_path=file_path, encoding='utf-8')\n",
    "                else: continue\n",
    "                \n",
    "                file_docs = loader.load()\n",
    "                documents.extend([add_metadata(doc, doc_type) for doc in file_docs])\n",
    "                print(f\"  ✓ Loaded {len(file_docs)} units from {file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ✗ Error loading {file_path}: {e}\")\n",
    "\n",
    "print(f\"\\n{'='*50}\\nTotal documents loaded: {len(documents)}\\n{'='*50}\")\n",
    "\n",
    "# Chunking logic exactly as you had it\n",
    "if documents:\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000, \n",
    "        chunk_overlap=200,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Total number of chunks: {len(chunks)}\")\n",
    "    \n",
    "    doc_type_counts = Counter(doc.metadata['doc_type'] for doc in documents)\n",
    "    chunk_type_counts = Counter(chunk.metadata['doc_type'] for chunk in chunks)\n",
    "    print(f\"\\nDocuments per type:\")\n",
    "    for d_type, count in doc_type_counts.items():\n",
    "        print(f\"  - {d_type}: {count} docs → {chunk_type_counts[d_type]} chunks\")\n",
    "\n",
    "\n",
    "if os.path.exists(db_name):\n",
    "    shutil.rmtree(db_name, ignore_errors=True)\n",
    "    print(f\"Deleted: {db_name}\")\n",
    "else:\n",
    "    print(\"Directory does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0fb0045-29a9-401a-a714-8031dd94eab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorstore created with 173 documents\n"
     ]
    }
   ],
   "source": [
    "# Put the chunks of data into a Vector Store that associates a Vector Embedding with each chunk\n",
    "# Chroma is a popular open source Vector Database based on SQLLite\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# If you would rather use the free Vector Embeddings from HuggingFace sentence-transformers\n",
    "# Then replace embeddings = OpenAIEmbeddings()\n",
    "# with:\n",
    "# from langchain.embeddings import HuggingFaceEmbeddings\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "# Delete if already exists\n",
    "if os.path.exists(db_name):\n",
    "    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()\n",
    "\n",
    "# Create vectorstore\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)\n",
    "collection = vectorstore._collection\n",
    "print(f\"Vectorstore created with {collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44bb67f-3aec-42c1-8fbf-c19872f5e29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory for context-aware chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 8})\n",
    "\n",
    "# Create the chain\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm, \n",
    "    retriever=retriever, \n",
    "    memory=memory,\n",
    "    callbacks=[StdOutCallbackHandler()] # This shows you the logic in the terminal\n",
    ")\n",
    "\n",
    "def chat_function(message, history):\n",
    "    # Gradio sends 'message' as a dict or string depending on version\n",
    "    query = message[\"text\"] if isinstance(message, dict) else message\n",
    "    result = conversation_chain.invoke({\"question\": query})\n",
    "    return result[\"answer\"]\n",
    "\n",
    "# Launch the UI\n",
    "demo = gr.ChatInterface(fn=chat_function, title=\"Kaiserslautern University Assistant\")\n",
    "demo.launch(inbrowser=True, share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d5130c-db33-4642-9139-477e5d2c5cab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Kaiserslautern RAG)",
   "language": "python",
   "name": "kaiserslautern_rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
